\chapter{Accurate Newton's Method for B\'{e}zier Curve Intersection}
\label{chap:compensated-newton}

\section{Introduction}

When using Newton's method to find the root of a function via
\begin{equation}
G\left(\bm{x}\right) = \bm{x} - J^{-1} F\left(\bm{x}\right)
\end{equation}
there are three computations performed that can introduce instability:
evaluation of the residual function \(F\left(\bm{x}\right)\), evaluation
of the Jacobian \(J\) and solution of the linear system \(J \bm{y} =
F\left(\bm{x}\right)\). In \cite{Tisseur2001}, the author showed that by
just using a more precise evaluation of the residual function, the
accuracy of Newton's method can be improved.

This chapter considers Newton's method applied to two problems:
root-finding for polynomials expressed in Bernstein form and
intersection of two B\'{e}zier curves in \(\reals^2\). In both problems,
the compensated de Casteljau method (see Chapter~\ref{chap:k-compensated}) is
used for evaluation of the residual. When evaluating a polynomial
\(p(s)\) this is straightforward, but when evaluating the difference
\(b_1(s) - b_2(t)\) between two curves special care must be taken.

\begin{figure}
  \includegraphics{../images/compensated-newton/newton_jghplus13.pdf}
  \centering
  \captionsetup{width=.75\linewidth}
  \caption{Comparing relative error to condition number when using Newton's
    method to find a root of \(p(s) = (s - 1)^n - 2^{-31}\), where polynomial
    evaluation occurs via Horner's method.}
  \label{fig:jgh+13}
\end{figure}

In \cite{Graillat2008}, the problem of finding simple roots \(\alpha\) of
polynomials \(p(s)\) expressed in the monomial basis is considered.
A standard Newton's method (\texttt{HNewtonBasic}) that evaluates \(p(s)\)
and \(p'(s)\) using Horner's method is compared to a modified Newton's method
(\texttt{HNewtonAccurate}) that evaluates \(p(s)\) with a compensated
Horner's method. This proceeds as in \cite{Tisseur2001}: the evaluation of
the residual is done with greater accuracy but the rest of the process
is the same. When computing a root \(\alpha\), the standard Newton's method
has a relative error that grows linearly with the condition number of the
root (which will be defined in Section~\ref{sec:conditioning}). The
modified Newton's method is fully accurate to machine precision (i.e.
the relative error is \(\bigO{\mach}\)) until \(\cond{\alpha}\) reaches
\(1/\mach\), as seen in Figure~\ref{fig:jgh+13}.

After the point where \texttt{HNewtonAccurate} loses accuracy, we'd
expect a linear increase in relative error based on a compensated
rule of thumb:
\begin{equation}
\frac{\left|\widehat{\alpha} - \alpha\right|}{\left|\alpha\right|} \leq
  c_1 \mach + c_2 \cond{\alpha} \mach^2
\end{equation}
where \(c_1 \mach\) corresponds to rounding into the given
precision and \(c_2 \cond{\alpha} \mach^2\) reflects the
typical error but from computations done with working precision
\(\overline{\mach} = \mach^2\). The point
where the condition number exceeds \(1/\mach\) should correspond to
the point where the second term is larger that the
first term. However, this is not possible unless the
Jacobian (i.e. \(p'(s)\)) is also evaluated with a compensated
method. A second modified Newton's method (\texttt{HNewtonFull}) is
introduced in \cite[Section~8]{Jiang2013} and the author shows that
this second modified Newton's method does indeed follow a compensated
rule of thumb under appropriate conditions. We see in
Figure~\ref{fig:jgh+13} that \texttt{HNewtonFull} enables
\(\bigO{\mach}\) relative errors until the condition number reaches
\(1/\mach\) and then a linear increase in error as the condition number
grows from \(1/\mach\) to \(1/\mach^2\).

We'll proceed similarly for polynomials in
Bernstein form. Since this is a one dimensional Newton's method, improving
the evaluation of the Jacobian is straightforward. The results agree with
what has been observed when using Horner's method for evaluation.

Computing the intersection(s) of two parametric plane curves is a
common task in computational geometry and has many uses, e.g. in
finite element methods that use overlapping curved meshes. Many
methods have been described in the literature to solve this problem.
Algebraic methods such as implicitization and eigenvalue-based
methods (e.g. \cite{Manocha:CSD-92-698}) suffer
from accuracy issues for moderately high degrees and can often be
very complex to implement. Some (\cite{Bates2008}) even rely on symbolic
algebraic manipulations, which can be quite costly since it requires
arbitrary precision.
Geometric methods (e.g. \cite{Sederberg1986, Sederberg1990, Kim1998})
typically use a form of domain splitting to focus on subproblems and
eliminate parts of the domain where an intersection is guaranteed not
to occur. After a domain has been sufficiently reduced, Newton's method
is used for the last few bits of accuracy.

We'll focus on transversal curve intersections that are ill-conditioned.
Transversal intersections are an extension of the concept of a
simple root. A non-transversal intersection occurs when the curves
or tangent at the point of intersection or when one of the curves
has a zero tangent vector at that point, either due to an improper
parameterization (e.g. \(x(s) = s^2, y(s) = s^2 + 1\)) or a cusp.
In many cases, transversal intersections that are ``almost tangent'' have
very high condition numbers.

The chapter is organized as follows. In Section~\ref{sec:conditioning}
we define and discuss the conditioning of both a simple root and
a transversal intersection. In Section~\ref{sec:compensated-simple-roots}
we describe two compensated Newton's methods for finding simple roots
and perform a numerical experiment verifying the expected behavior.
In Section~\ref{sec:compensated-curve-intersect} we describe a
compensated Newton's method for B\'{e}zier curve intersection and
perform sever numerical experiements to verify the expected behavior
on both transversal intersections and tangent intersections (i.e.
intersections with infinite condition number).
Section~\ref{sec:false-starts} acts as a
coda: it describes some failed attempts at constructing numerical
examples. This section provides an in-depth discussion of a particular
family of polynomials that has much better than expected conditioning
when written in the Bernstein basis.

\section{Problem conditioning}\label{sec:conditioning}

Consider a smooth function \(F: \reals^n \longrightarrow \reals^n\)
with Jacobian \(F_{\bm{x}} = J\). We want to consider a special class of
functions of the form \(F\left(\bm{x}\right) = \sum_j c_j
\phi_j\left(\bm{x}\right)\) where the basis
functions \(\phi_j\) are also smooth functions on \(\reals^n\)
and each \(c_j \in \reals\). We want to consider the effects on a root
\(\bm{\alpha} \in \reals^n\) of a perturbation in one of the
coefficients \(c_j\). We examine the perturbed functions
\begin{equation}
G(x, \delta) = F\left(\bm{x}\right) + \delta \phi_j\left(\bm{x}\right).
\end{equation}
Since \(G\left(\bm{\alpha}, 0\right) = \bm{0}\), if \(J^{-1}\) exists at
\(\bm{x} = \bm{\alpha}\) then
the implicit function theorem tells us that we can define
\(\bm{x}\) via
\begin{equation}
G\left(\bm{x}\left(\delta\right), \delta\right) = \bm{0}.
\end{equation}
Taking the derivative with respect to \(\delta\) we find that
\(\bm{0} = G_{\bm{x}} \bm{x}_{\delta} + G_{\delta}\). Plugging in
\(\delta = 0\) we find that \(0 = J\left(\bm{\alpha}\right) \bm{x}_{\delta} +
\phi_j\left(\bm{\alpha}\right)\), hence we
conclude that
\begin{equation}
\bm{x}\left(\delta\right) = \bm{\alpha} - J\left(\bm{\alpha}\right)^{-1}
  \phi_j\left(\bm{\alpha}\right) \delta + \bigO{\delta^2}.
\end{equation}
This gives a relative condition number (for the root) of
\begin{equation}
\frac{\left \lVert J\left(\bm{\alpha}\right)^{-1}
  \phi_j\left(\bm{\alpha}\right) \right \rVert}{
  \left \lVert \bm{\alpha} \right \rVert}.
\end{equation}

By considering perturbations in \textbf{all} of the coefficients:
\(\left|\delta_j\right| \leq \eps \left|c_j\right|\), a similar analysis
gives a root function
\begin{equation}
\bm{x}\left(\delta_0, \ldots, \delta_n\right) = \bm{\alpha} -
  J\left(\bm{\alpha}\right)^{-1} \sum_{j = 0}^n \delta_j
  \phi_j\left(\bm{\alpha}\right) + \bigO{\eps^2}.
\end{equation}
With this, we can define a root condition number
\begin{equation}\label{eq:abstract-cond-num}
\kappa_{\bm{\alpha}} =
  \lim_{\eps \to 0} \left(\sup \frac{\left \lVert\delta \bm{\alpha}
  \right \rVert / \eps}{\left \lVert\bm{\alpha}\right \rVert}\right) =
  \lim_{\eps \to 0} \left(\sup \frac{\left \lVert
  J\left(\bm{\alpha}\right)^{-1} \sum_j \delta_j
  \phi_j\left(\bm{\alpha}\right) \right \rVert / \eps}{
  \left \lVert\bm{\alpha}\right \rVert}\right).
\end{equation}

When \(n = 1\), \(J^{-1}\) is simply \(1 / F'\) and we find
\begin{equation}
\kappa_{\alpha} =
  \frac{1}{\left|\alpha F'(\alpha)\right|} \sum_{j = 0}^n \left|
  c_j \phi_j(\alpha)\right|.
\end{equation}
This value is given by the triangle inequality applied to
\(\delta \alpha\)  and equality can be attained since the sign
of each \(\delta_j = \pm c_j \eps\) can be modified at will to make
\(\phi_j(\alpha) \delta_j = \left|\phi_j(\alpha) c_j\right| \eps\).

When \(n > 1\), the triangle inequality tells us that
\begin{equation}
\kappa_{\bm{\alpha}} =
  \lim_{\eps \to 0} \left(\sup \frac{\left \lVert\delta \bm{\alpha} /
  \eps\right \rVert}{\left \lVert\bm{\alpha}\right \rVert}\right) \leq
  \frac{1}{\left \lVert\bm{\alpha}\right \rVert} \sum_{j = 0}^n
  \left|c_j\right| \left \lVert J\left(\bm{\alpha}\right)^{-1}
  \phi_j(\bm{\alpha})\right \rVert.
\end{equation}
However, this bound is only attainable if all
\(\phi_j(\bm{\alpha})\) are parallel. However, we'll seldom need to
compute the exact condition number and are instead typically
interested in the order of magnitude. In this case a lower
bound
\begin{equation}
\frac{1}{\left \lVert\bm{\alpha}\right \rVert}
\max_j \left|c_j\right| \left \lVert J\left(\bm{\alpha}\right)^{-1}
\phi_j(\bm{\alpha})\right \rVert
\end{equation}
for \(\kappa_{\bm{\alpha}}\)
will suffice as an approximate condition number.

For an example, consider
\begin{equation}
\phi_0 = \left[ \begin{array}{c} x_0 \\ 2 \\ 0 \end{array}\right],
\phi_1 = \left[ \begin{array}{c} 0 \\ x_1 \\ 3 \end{array}\right],
\phi_2 = \left[ \begin{array}{c} 2 \\ 0 \\ x_2 \end{array}\right],
F = \phi_0 + 2 \phi_1 + 3 \phi_2,
\bm{\alpha} = \left[ \begin{array}{c} -6 \\ -1 \\ -2 \end{array}\right].
\end{equation}
For a given \(\eps\), the maximum root perturbation occurs when
\(\delta_0 = \eps, \delta_1 = 2 \eps, \delta_2 = -3 \eps\) and
gives
\(\left \lVert J\left(\bm{\alpha}\right)^{-1} \sum_j
\delta_j \phi_j\left(\bm{\alpha}\right) \right \rVert
= 4 \sqrt{10} \eps \approx 12.65 \eps\).
The pessimistic triangle inequality bound gives
\(\sum_j \left|c_j\right| \left \lVert J\left(\bm{\alpha}\right)^{-1}
\phi_j(\bm{\alpha})\right \rVert \approx 14.64 \eps\) and the
maximum individual perturbation is \(2 \sqrt{10} \eps \approx 6.325 \eps\)
(this occurs when \(\delta_0 = \delta_1 = 0, \delta_2 = \pm 3 \eps\)).

In this general framework, we can define a condition number both
for a simple root of a polynomial in Bernstein form and for the
intersection of two planar B\'{e}zier curves. For the first,
\(\phi_j(s) = \binom{n}{j} (1 - s)^{n - j} s^j\) the Bernstein basis
functions, a polynomial \(p(s) = \sum_j b_j \phi_j(s)\) with
a simple root \(\alpha \in \left(0, 1\right]\) has root condition number
\begin{equation}
\kappa_{\alpha} =
  \frac{1}{\alpha \left|p'(\alpha)\right|} \sum_{j = 0}^n \left|
  b_j \phi_j(\alpha)\right| = \frac{\widetilde{p}(\alpha)}{
  \alpha \left|p'(\alpha)\right|}.
\end{equation}
For the intersection of a degree \(m\) curve \(b_1(s)\) and
a degree \(n\) curve \(b_2(t)\), we have basis functions
\begin{multline}
\phi_{0, -1, 1} = \left[ \begin{array}{c} B_{0, m}(s) \\ 0 \end{array}\right],
\phi_{0, -1, 2} = \left[ \begin{array}{c} 0 \\ B_{0, m}(s) \end{array}\right],
\cdots, \\
\phi_{m, -1, 1} = \left[ \begin{array}{c} B_{m, m}(s) \\ 0 \end{array}\right],
\phi_{m, -1, 2} = \left[ \begin{array}{c} 0 \\
  B_{m, m}(s) \end{array}\right], \\
\phi_{-1, 0, 1} = \left[ \begin{array}{c} -B_{0, n}(t) \\
  0 \end{array}\right],
\phi_{-1, 0, 2} = \left[ \begin{array}{c} 0 \\
  -B_{0, n}(t) \end{array}\right], \cdots, \\
\phi_{-1, n, 1} = \left[ \begin{array}{c} -B_{n, n}(t) \\
  0 \end{array}\right], \phi_{-1, n, 2} = \left[ \begin{array}{c} 0 \\
  -B_{n, n}(t) \end{array}\right].
\end{multline}
Since \(F(s, t) = b_1(s) - b_2(t)\) we have Jacobian \(J(s, t) =
\left[ \begin{array}{c c} b_1'(s) & -b_2'(t) \end{array}\right]\). We'll
consider a transversal intersection \(F(\alpha, \beta) = \bm{0}\) with
\(\det J(\alpha, \beta) \neq 0\). Since each of the
\(\phi_j\) is just a scalar multiple of the standard basis
vectors, writing \(J^{-1} = \left[ \begin{array}{c c}
\bm{v}_1 & \bm{v}_2 \end{array}\right]\), we have
\begin{multline}
J\left(\alpha, \beta\right)^{-1} \sum_{\bm{j}} \delta_{\bm{j}}
  \phi_{\bm{j}}\left(\alpha, \beta\right) = \left[\sum_{i = 0}^m
  \delta_{i, -1, 1} B_{i, m}\left(\alpha\right) + \sum_{j = 0}^n
  \delta_{-1, j, 1} B_{j, n}\left(\beta\right)\right] \bm{v}_1 \\
+ \left[\sum_{i = 0}^m
  \delta_{i, -1, 2} B_{i, m}\left(\alpha\right) + \sum_{j = 0}^n
  \delta_{-1, j, 2} B_{j, n}\left(\beta\right)\right] \bm{v}_2 =
  \nu_1 \bm{v}_1 + \nu_2 \bm{v}_2.
\end{multline}
where
\begin{equation}\label{eq:nu-bounds}
\left|\nu_k\right| / \eps \leq \sum_{i = 0}^m
  \left|c_{i, -1, k}\right| B_{i, m}\left(\alpha\right) + \sum_{j = 0}^n
  \left|c_{-1, j, k}\right| B_{j, n}\left(\beta\right)
\end{equation}
and the bound can be attained for both \(k = 1, 2\) by making the
signs of the \(\delta_{\bm{j}}\) agree. Thus we have condition
number
\begin{equation}\label{eq:intersect-cond-num}
\kappa_{\alpha, \beta} = \frac{1}{\sqrt{\alpha^2 + \beta^2}}
  \max_{\nu_1, \nu_2} \left \lVert \nu_1 \bm{v}_1 +
  \nu_2 \bm{v}_2 \right \rVert_2
\end{equation}
where the domain is restricted to the rectangle defined
by~\eqref{eq:nu-bounds} (and setting \(\eps = 1\) in that bound
due to cancellation in~\eqref{eq:abstract-cond-num}). Since
\(J^{-1}\) is invertible, we know \(\bm{v}_1\) and \(\bm{v}_2\) are
not parallel hence we know the maximum in~\eqref{eq:intersect-cond-num}
occurs at one of the four corners of the rectangle or at one of at
most four critical points along the boundary.

As far as the author can tell,
a condition number for the intersection of two planar B\'{e}zier curves
has not been described in the Computer Aided Geometric Design (CAGD)
literature. In \cite[Chapter~25, Equation 25.11]{Higham2002}
a more generic condition number is defined for the root of a nonlinear
algebraic system that is similar to the definition above.

\section{Simple polynomial roots}\label{sec:compensated-simple-roots}

Placeholder.

\section{B\'{e}zier curve intersection}\label{sec:compensated-curve-intersect}

Placeholder.

\section{False starts}\label{sec:false-starts}

Placeholder.
